{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET = r'/media/gagandeep/2E92405C92402AA3/Work/Kaggle/Zips/fake-news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['submit.csv', 'test.csv', 'train.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "os.listdir(PATH_TO_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id                                              title              author  \\\n0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n\n                                                text  label  \n0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n1  Ever get the feeling your life circles the rou...      0  \n2  Why the Truth Might Get You Fired October 29, ...      1  \n3  Videos 15 Civilians Killed In Single US Airstr...      1  \n4  Print \\nAn Iranian woman has been sentenced to...      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>author</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>Darrell Lucus</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n      <td>Daniel J. Flynn</td>\n      <td>Ever get the feeling your life circles the rou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Why the Truth Might Get You Fired</td>\n      <td>Consortiumnews.com</td>\n      <td>Why the Truth Might Get You Fired October 29, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n      <td>Jessica Purkiss</td>\n      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Iranian woman jailed for fictional unpublished...</td>\n      <td>Howard Portnoy</td>\n      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(PATH_TO_DATASET, 'train.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape of features: (18285, 4)\nShape of labels: (18285,)\n"
    }
   ],
   "source": [
    "# Get features and label from the data\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "print(\"Shape of features: {}\".format(X.shape))\n",
    "print(\"Shape of labels: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal is to build a LSTM model - unidirectional and biderectional to classify fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the vocabulary size -> the size of our corpus.\n",
    "voc_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# One hot representation\n",
    "messages = X.copy()\n",
    "messages['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/gagandeep/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 18285/18285 [00:35<00:00, 510.76it/s]\n"
    }
   ],
   "source": [
    "# Pre process the data\n",
    "# The same steps will be used when making predictions\n",
    "import tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in tqdm.tqdm(range(0, len(messages))):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['hous dem aid even see comey letter jason chaffetz tweet',\n 'flynn hillari clinton big woman campu breitbart',\n 'truth might get fire',\n 'civilian kill singl us airstrik identifi',\n 'iranian woman jail fiction unpublish stori woman stone death adulteri',\n 'jacki mason hollywood would love trump bomb north korea lack tran bathroom exclus video breitbart',\n 'beno hamon win french socialist parti presidenti nomin new york time',\n 'back channel plan ukrain russia courtesi trump associ new york time',\n 'obama organ action partner soro link indivis disrupt trump agenda',\n 'bbc comedi sketch real housew isi caus outrag']"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(corpus)\n",
    "y = np.array(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer(num_words=voc_size)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "\n",
    "# Now to make the length of each sentance uniform, we will define a fixed length and pad all the sentances which are less than the ficed length value\n",
    "fixed_length = 20\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=fixed_length) \n",
    "import pickle\n",
    "\n",
    "po = open('tokenizer.pkl', 'wb')\n",
    "pickle.dump(tok, po)\n",
    "po.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 20, 40)            200000    \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 20, 40)            0         \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 100)               56400     \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 256,501\nTrainable params: 256,501\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Model Creation - Unidirectional\n",
    "embedding_dim = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_dim, input_length=fixed_length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tok.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, padding='pre', maxlen=fixed_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(corpus)\n",
    "sequences = tok.texts_to_sequences(corpus)\n",
    "\n",
    "# Now to make the length of each sentance uniform, we will define a fixed length and pad all the sentances which are less than the ficed length value\n",
    "fixed_length = 20\n",
    "embedded_docs = pad_sequences(encoded_corpus, padding='pre', maxlen=fixed_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 12799 samples, validate on 5486 samples\nEpoch 1/15\n12799/12799 [==============================] - 6s 484us/sample - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.5601 - val_accuracy: 0.9213\nEpoch 2/15\n12799/12799 [==============================] - 6s 480us/sample - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.4048 - val_accuracy: 0.9200\nEpoch 3/15\n12799/12799 [==============================] - 7s 514us/sample - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.4076 - val_accuracy: 0.9121\nEpoch 4/15\n12799/12799 [==============================] - 6s 439us/sample - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.5976 - val_accuracy: 0.9209\nEpoch 5/15\n12799/12799 [==============================] - 5s 424us/sample - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.5907 - val_accuracy: 0.9203\nEpoch 6/15\n12799/12799 [==============================] - 6s 481us/sample - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.6131 - val_accuracy: 0.9214\nEpoch 7/15\n12799/12799 [==============================] - 6s 496us/sample - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.9007 - val_accuracy: 0.9171\nEpoch 8/15\n12799/12799 [==============================] - 7s 520us/sample - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.7049 - val_accuracy: 0.9167\nEpoch 9/15\n12799/12799 [==============================] - 7s 521us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.6540 - val_accuracy: 0.9209\nEpoch 10/15\n12799/12799 [==============================] - 9s 674us/sample - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.7934 - val_accuracy: 0.9171\nEpoch 11/15\n12799/12799 [==============================] - 7s 563us/sample - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.7763 - val_accuracy: 0.9196\nEpoch 12/15\n12799/12799 [==============================] - 8s 639us/sample - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.9006 - val_accuracy: 0.9183\nEpoch 13/15\n12799/12799 [==============================] - 7s 531us/sample - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.8214 - val_accuracy: 0.9187\nEpoch 14/15\n12799/12799 [==============================] - 8s 606us/sample - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.9449 - val_accuracy: 0.9185\nEpoch 15/15\n12799/12799 [==============================] - 7s 541us/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.6808 - val_accuracy: 0.9182\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f85400fc828>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(embedded_docs, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1],\n       [0],\n       [0],\n       ...,\n       [1],\n       [0],\n       [1]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------Confusion Matrix -----------------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[2835,  272],\n       [ 177, 2202]])"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "print(\"-------------Confusion Matrix -----------------\")\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9181553044112286"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------Classification Report -----------------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'              precision    recall  f1-score   support\\n\\n           0       0.94      0.91      0.93      3107\\n           1       0.89      0.93      0.91      2379\\n\\n    accuracy                           0.92      5486\\n   macro avg       0.92      0.92      0.92      5486\\nweighted avg       0.92      0.92      0.92      5486\\n'"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "print(\"-------------Classification Report -----------------\")\n",
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_Uni_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "# Model Creation - Biidirectional\n",
    "embedding_dim = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_dim, input_length=fixed_length))\n",
    "model.add(Bidirectional(LSTM(100, activation='relu')))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 12799 samples, validate on 5486 samples\nEpoch 1/30\n12799/12799 [==============================] - 16s 1ms/sample - loss: 0.3202 - accuracy: 0.8459 - val_loss: 0.2478 - val_accuracy: 0.9140\nEpoch 2/30\n12799/12799 [==============================] - 11s 821us/sample - loss: 0.1523 - accuracy: 0.9465 - val_loss: 0.1926 - val_accuracy: 0.9234\nEpoch 3/30\n12799/12799 [==============================] - 11s 855us/sample - loss: 0.0902 - accuracy: 0.9673 - val_loss: 0.2219 - val_accuracy: 0.9149\nEpoch 4/30\n12799/12799 [==============================] - 11s 862us/sample - loss: 0.0635 - accuracy: 0.9787 - val_loss: 0.2587 - val_accuracy: 0.9213\nEpoch 5/30\n12799/12799 [==============================] - 11s 849us/sample - loss: 0.0453 - accuracy: 0.9850 - val_loss: 0.2855 - val_accuracy: 0.9196\nEpoch 6/30\n12799/12799 [==============================] - 11s 865us/sample - loss: 0.0306 - accuracy: 0.9905 - val_loss: 0.3273 - val_accuracy: 0.9182\nEpoch 7/30\n12799/12799 [==============================] - 11s 889us/sample - loss: 0.0237 - accuracy: 0.9932 - val_loss: 0.4351 - val_accuracy: 0.9174\nEpoch 8/30\n12799/12799 [==============================] - 11s 872us/sample - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.5029 - val_accuracy: 0.9171\nEpoch 9/30\n12799/12799 [==============================] - 11s 833us/sample - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.5419 - val_accuracy: 0.9151\nEpoch 10/30\n12799/12799 [==============================] - 11s 850us/sample - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.4168 - val_accuracy: 0.9185\nEpoch 11/30\n12799/12799 [==============================] - 11s 838us/sample - loss: 0.0154 - accuracy: 0.9958 - val_loss: 0.3552 - val_accuracy: 0.9196\nEpoch 12/30\n12799/12799 [==============================] - 11s 835us/sample - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.5309 - val_accuracy: 0.9143\nEpoch 13/30\n12799/12799 [==============================] - 11s 844us/sample - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.7302 - val_accuracy: 0.9176\nEpoch 14/30\n12799/12799 [==============================] - 11s 830us/sample - loss: 0.0057 - accuracy: 0.9981 - val_loss: 1.2010 - val_accuracy: 0.9165\nEpoch 15/30\n12799/12799 [==============================] - 11s 836us/sample - loss: 0.0040 - accuracy: 0.9987 - val_loss: 1.8428 - val_accuracy: 0.9171\nEpoch 16/30\n12799/12799 [==============================] - 11s 836us/sample - loss: 0.0054 - accuracy: 0.9985 - val_loss: 3.2398 - val_accuracy: 0.9136\nEpoch 17/30\n12799/12799 [==============================] - 11s 874us/sample - loss: 0.0045 - accuracy: 0.9986 - val_loss: 6.3087 - val_accuracy: 0.9145\nEpoch 18/30\n12799/12799 [==============================] - 11s 854us/sample - loss: 0.0109 - accuracy: 0.9974 - val_loss: 9.0567 - val_accuracy: 0.9129\nEpoch 19/30\n12799/12799 [==============================] - 11s 833us/sample - loss: 0.0598 - accuracy: 0.9835 - val_loss: 41.9187 - val_accuracy: 0.9162\nEpoch 20/30\n12799/12799 [==============================] - 11s 824us/sample - loss: 0.0455 - accuracy: 0.9923 - val_loss: 0.5865 - val_accuracy: 0.9121\nEpoch 21/30\n12799/12799 [==============================] - 11s 838us/sample - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.7873 - val_accuracy: 0.9134\nEpoch 22/30\n12799/12799 [==============================] - 11s 859us/sample - loss: 0.0057 - accuracy: 0.9991 - val_loss: 0.8124 - val_accuracy: 0.9145\nEpoch 23/30\n12799/12799 [==============================] - 11s 868us/sample - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.8464 - val_accuracy: 0.9156\nEpoch 24/30\n12799/12799 [==============================] - 11s 843us/sample - loss: 0.0029 - accuracy: 0.9993 - val_loss: 1.1130 - val_accuracy: 0.9118\nEpoch 25/30\n12799/12799 [==============================] - 12s 960us/sample - loss: 0.0036 - accuracy: 0.9991 - val_loss: 1.7090 - val_accuracy: 0.9136\nEpoch 26/30\n12799/12799 [==============================] - 12s 907us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 1.3124 - val_accuracy: 0.9138\nEpoch 27/30\n12799/12799 [==============================] - 12s 904us/sample - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.6947 - val_accuracy: 0.9149\nEpoch 28/30\n12799/12799 [==============================] - 11s 835us/sample - loss: 0.0246 - accuracy: 0.9945 - val_loss: 0.5766 - val_accuracy: 0.9141\nEpoch 29/30\n12799/12799 [==============================] - 11s 891us/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.1303 - val_accuracy: 0.9149\nEpoch 30/30\n12799/12799 [==============================] - 12s 928us/sample - loss: 4.4876e-04 - accuracy: 0.9999 - val_loss: 1.3419 - val_accuracy: 0.9163\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f852b6ede10>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(embedded_docs, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1],\n       [0],\n       [0],\n       ...,\n       [1],\n       [0],\n       [1]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------Confusion Matrix -----------------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[2794,  313],\n       [ 146, 2233]])"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "print(\"-------------Confusion Matrix -----------------\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9163324826831936"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------Classification Report -----------------\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'              precision    recall  f1-score   support\\n\\n           0       0.95      0.90      0.92      3107\\n           1       0.88      0.94      0.91      2379\\n\\n    accuracy                           0.92      5486\\n   macro avg       0.91      0.92      0.92      5486\\nweighted avg       0.92      0.92      0.92      5486\\n'"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "print(\"-------------Classification Report -----------------\")\n",
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_Bi_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bite6955f2e74d64707b01ddae88c9da161",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}